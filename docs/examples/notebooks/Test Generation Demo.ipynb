{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eec8fb5c-b66d-4cd1-9432-05678cde3607",
   "metadata": {},
   "source": [
    "# Using CLDK to generate JUnit tests\n",
    "\n",
    "\n",
    "In this tutorial, we will use CLDK to generate a JUnit test for all the methods in a Java Application. \n",
    "\n",
    "By the end of this tutorial, you will have a JUnit test for all the methods in a Java application. You'll be able to explore some of benefits of using CLDK to perform fast and easy program analysis and build a LLM-based test generator.\n",
    "\n",
    "You will learn how to do the following:\n",
    "\n",
    "1. Create a new instance of the CLDK class.\n",
    "2. Create an analysis object over the Java application.\n",
    "3. Iterate over all the files in the project.\n",
    "4. Iterate over all the classes in the file.\n",
    "5. Iterate over all the methods in the class.\n",
    "6. Get the code body of the method.\n",
    "7. Initialize the treesitter utils for the class file content.\n",
    "8. Sanitize the class for analysis.\n",
    "\n",
    "Next, we will write a couple of helper methods to:\n",
    "1.  Format the instruction for the given focal method and class\n",
    "2.  Prompts the local model on Ollama \n",
    "3.  Prints the instruction and LLM output \n",
    "\n",
    "## Prequisites \n",
    "\n",
    "Before we get started, let's make sure you have the following installed:\n",
    "\n",
    "1. Python 3.11 or later\n",
    "2. Ollama 0.3.4 or later\n",
    "   \n",
    "We will use ollama to spin up a local granite model that will act as our LLM for this turorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6148e6e1",
   "metadata": {},
   "source": [
    "### Prerequisite 1: Install ollama\n",
    "\n",
    "If you don't have ollama installed, please download and install it from here: [Ollama](https://ollama.com/download). \n",
    "\n",
    "Once you have ollama, start the server and make sure it is running.\n",
    "\n",
    "If you're on MacOS, Linux, or WSL, you can check to make sure the server is running by running the following command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0070dbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "systemctl status ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db92ae11",
   "metadata": {},
   "source": [
    "If not, you may have to start the server manually. You can do this by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1cadd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "systemctl start ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404b2174",
   "metadata": {},
   "source": [
    "### Prerequisite 2: Pull the latest version of Granite 8b Instruct Model\n",
    "\n",
    "Once ollama is up and running, you can download the latest version of the Granite 8b Instruct model by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5006aef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ollama pull granite-code:8b-instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175ae11b",
   "metadata": {},
   "source": [
    "There are other granite versions available, but for this tutorial, we will use the Granite 8b Instruct model. You if prefer to use a different version, you can replace `8b-instruct` with any of the other [versions](https://ollama.com/library/granite-code/tags).\n",
    "\n",
    "Let's make sure the model is downloaded by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25436f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ollama run granite-code:8b-instruct \"Write a python function to print 'Hello, World!'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0c3a94",
   "metadata": {},
   "source": [
    "### Prerequisite 3: Install ollama Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30f5139",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3049cc",
   "metadata": {},
   "source": [
    "### Prerequisite 4: Install CLDK\n",
    "\n",
    "CLDK is avaliable on github at github.com/IBM/codellm-devkit.git. You can install it by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3721cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install git+https://github.com/IBM/codellm-devkit.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7899a617",
   "metadata": {},
   "source": [
    "## Building a JUnit test generator using CLDK and Granite Code Instruct Model\n",
    "\n",
    "Now that we have all the prerequisites installed, let's start building a JUnit test generator using CLDK and the Granite Code Instruct Model.\n",
    "\n",
    "### Step 1: Get the sample Java application\n",
    "\n",
    "For this tutorial, we will use apache commons cli. You can download the source code to a temporary directory by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c13c49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget https://github.com/apache/commons-cli/archive/refs/tags/rel/commons-cli-1.7.0.zip -O /tmp/commons-cli-1.7.0.zip && unzip -o /tmp/commons-cli-1.7.0.zip -d /tmp "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb3251a",
   "metadata": {},
   "source": [
    "The project will now be extracted to `/tmp/commons-cli-rel-commons-cli-1.7.0`. We'll remove these files later, so don't worry about the location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b152d0e",
   "metadata": {},
   "source": [
    "### Step 2: Create a new instance of the CLDK class\n",
    "\n",
    "Let's import CLDK and create a new analysis object over the Java application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23036887",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "from cldk import CLDK\n",
    "\n",
    "\n",
    "cldk = CLDK(language='java')\n",
    "java_analysis = cldk.analysis(project_path=\"/tmp/commons-cli-rel-commons-cli-1.7.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8462099b",
   "metadata": {},
   "source": [
    "Now, let's get all the methods in the project! We will use the `get_methods()` method to get all the methods in the project. This will return dictionary where the key is the class name the values are a map of method signatures to their callable objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32a7a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_methods = java_analysis.get_methods()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dabb60",
   "metadata": {},
   "source": [
    "Now, that we have a dictionary of all the methods in the project, let's write a couple of quick helpers that will prompt the local model on Ollama and print the instruction and LLM output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1ce80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_an_prompt(method, reference_type):\n",
    "    \"\"\"A simple function to create an instruction for a method\"\"\"\n",
    "    inst = f\"Write a summary for the method {method} in the class {reference_type}\"\n",
    "    inst += f\"\\n```\\n{method.code}\\n```\"\n",
    "    return inst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247ac94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def execute_prompt(prompt: str):\n",
    "    \"\"\"A simple function to execute a prompt\"\"\"\n",
    "    return ollama.generate(model=\"granite-code:8b-instruct\", prompt=prompt)[\"response\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433ff15e",
   "metadata": {},
   "source": [
    "Finally, we will iterate over all the methods in `all_methods` and generate a JUnit test for each method. We will use the `format_instruction` method to format the instruction for the given focal method and class. We will then prompt the local model on Ollama and print the instruction and LLM output with the `prompt_local_model` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f052958c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "for type_name, method_dict in all_methods.items():\n",
    "    for method_signature, method in method_dict.items():\n",
    "        print(f\"Method: {method.code}\")\n",
    "        prompt = create_an_prompt(method, type_name)\n",
    "        response = execute_prompt(prompt)\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"response: {response}\")        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
